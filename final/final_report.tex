%------------------------------------------------------------------------------
%   PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%------------------------------------------------------------------------------

\documentclass[twoside,twocolumn]{article}

%\usepackage[sc]{mathpazo} % Use the Palatino font
%\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
%\linespread{1.05} % Line spacing - Palatino needs more space between lines
%\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage[english]{babel} % Language hyphenation and typographical rules

% Document margins
\usepackage[hmarginratio=1:1,top=32mm,left=20mm,right=20mm,columnsep=20pt]{geometry}
% Custom captions under/above floats in tables or figures
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption}
\usepackage{booktabs} % Horizontal rules in tables

\usepackage{enumitem} % Customized lists
\setlist[itemize]{noitemsep} % Make itemize lists more compact
\usepackage{textcomp}

% Allows abstract customization
\usepackage{abstract}
% Set the "Abstract" text to bold
\renewcommand{\abstractnamefont}{\normalfont\bfseries}
% Set the abstract itself to small italic text
\renewcommand{\abstracttextfont}{\normalfont\small\itshape}

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{\thetitle}
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\usepackage{titling} % Customizing the title section

\usepackage{hyperref} % For hyperlinks in the PDF
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{tikz}
\usetikzlibrary{bayesnet, arrows, positioning, fit, arrows.meta, shapes}

\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{graphicx}


\captionsetup[figure]{labelfont={bf},textfont=normalfont}

%------------------------------------------------------------------------------
%   TITLE SECTION
%------------------------------------------------------------------------------
\newlength\mystoreparindent
\newenvironment{myparindent}[1]{%
  \setlength{\mystoreparindent}{\the\parindent}
  \setlength{\parindent}{#1}
  }{%
  \setlength{\parindent}{\mystoreparindent}
}

\setlength{\droptitle}{-4\baselineskip} % Move the title up

\pretitle{\begin{center}\Huge\bfseries} % Article title formatting
\posttitle{\end{center}} % Article title closing formatting

\title{DL Project Final Report:\\
       Shakespeare - English Sequence to Sequence Modeling}
\author{%
\textsc{Morris Kraicer} \\[1ex]
\normalsize Johns Hopkins University \\
\normalsize \href{mailto:mkraice1@jhu.edu}{mkraice1@jhu.edu}
 \and
 \textsc{Riley Scott} \\[1ex]
\normalsize Johns Hopkins University \\
\normalsize \href{mailto:rscott39@jhu.edu}{rscott39@jhu.edu}
 \and
  \textsc{William Watson} \\[1ex]
\normalsize Johns Hopkins University \\
\normalsize \href{mailto:billwatson@jhu.edu}{billwatson@jhu.edu}
}

\date{}%\today} % Leave empty to omit a date
% \renewcommand{\maketitlehookd}{%

% }

%------------------------------------------------------------------------------

\begin{document}

% Print the title
\maketitle

%------------------------------------------------------------------------------
%   ARTICLE CONTENTS
%------------------------------------------------------------------------------

%------------------------------------------------

\begin{abstract}
% high level
\noindent
We present our attempt to create an English-Shakespeare Sequence to Sequence
Model, and its application to create a pseudo supervised style transfer system
for text. We experimented with different Encoder-Decoder Architectures with and
without Attention Mechanisms. We discuss our data procurement and processing
to help facilitate learning, and present our experimental results and analysis.
\end{abstract}

%------------------------------------------------
\section{Introduction}
We applied a suite of Encoder-Decoder models with varying attention
mechanisms (and lack of attention mechanisms) along with teacher forcing rates
to a corpus of Modern English - Original Shakespeare data. Discussion of our
data is in Section \ref{sec:data},
our processing methods are described in Section \ref{sec:preprocess}.
We used our neural machine translation systems to
apply a supervised style transfer between the two forms of writing.
More specifically, we tested models currently
used in decoding languages (Section \ref{sec:models}) and how they can
be used in this capacity. We provide results for several model
combinations and discuss the successes and failures of each system
(Section \ref{sec:results}). We also describe important implementation details
in Section \ref{sec:implementation}.
Diagrams for our models can be found in the appendix.
%------------------------------------------------
\section{Data}
\label{sec:data}
Most of our data came from previous alignment work
by Xu et al.\cite{xu2012paraphrasing}, in which
two aligned corpora were procured from Sparknotes and Enotes. Both datasets,
deriving from different websites, differed drastically. Sparknotes was more
liberal in its translations, opting for reordering that differed drastically
from the original plays, and was harder to learn. Enotes was a smaller corpus
of 8 plays, but had less reordering, and was easier to train on.
The data was aligned, and not all of the original lines
from the plays are included (i.e. the sentences could not be aligned properly).
\footnote{\url{https://github.com/cocoxu/Shakespeare}}

\begin{figure}[ht]
    \centering
    \begin{tabular}{ |l|r| }
        \hline
        \textbf{Play}
          & \textbf{Line Count} \\
        \hline
        Hamlet & 2,010 \\ \hline
        Julius Caesar & 1,201 \\ \hline
        Macbeth & 1,085 \\ \hline
        Merchant of Venice & 831 \\ \hline
        Midsummer Nights Dream & 833 \\ \hline
        Othello & 1,893 \\ \hline
        Romeo and Juliet & 1,743 \\ \hline
        Tempest & 769 \\ \hline
        \textbf{Total} & \textbf{10,365} \\ \hline
    \end{tabular}
    \caption{Line Counts for Shakespeare--English Corpus per Play (Enotes)}
    \label{fig:data-lines-enotes}
\end{figure}

The vocabulary sizes are 9,004 source (original) words and
7,497 target (modern) words for the Sparknotes set. We had a total of 10,365
lines of Shakespeare plays to train on. There is a total word count of 233,282
words for the entire corpus (unprocessed). Sample data pairs (processed) are
provided in Figure \ref{fig:sample-pairs}.

BLEU scores (disscussed in Section \ref{sec:bleu}) for Original to Modern is
0.4455, and Modern to Original is 0.4465, indicating strong correlation between
the two corpora.
\begin{figure*}[ht]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{ |c|l|l| }
      \hline
          $\#$
          & \textbf{Original Sentence}
          & \textbf{Modern Sentence}\\
      \hline
      1 & take me with you , take me with you , wife . & catch me , catch me , wife . \\
      \hline
      2 & that 's fouler . & that 's even more evil . \\
      \hline
      3 & perchance till after propn ' wedding - day . & maybe until after propn ' wedding - day . \\
      \hline
      4 & propn truly knows that thou art false as hell . & propn truly knows that you are as false as hell . \\
      \hline
      5 & then weep no more . & so stop crying . \\
      \hline
      6 & of propn , a n't please your mastership . & of propn , if it pleases you , sir . \\
      \hline
      7 & how ill this taper burns ! & how badly this candle burns ! \\
      \hline
      8 & the charm 's wound up . & the charm ’ s going to bring things to a head . \\
      \hline
      9 & before the time be out ? & before the debt is paid ? \\
      \hline
      10 & but soft ! & quiet ! \\
      \hline
      \end{tabular}}
  \caption{Sample Original-Modern Sentence Pairs, Processed}
  \label{fig:sample-pairs}
\end{figure*}
%------------------------------------------------
\section{Preprocessing}
\label{sec:preprocess}
We applied several techniques to improve the quality of our data and reduce the
vocabulary size to train on.
\paragraph{Proper Nouns:}
Proper nouns are unique in both corpus, and have direct translations. In order
to reduce vocabulary size and aggregate the learning of all proper nouns,
we replace all proper nouns with the following token: propn. Thus our model
should learn to map propn to propn, and can utilize the encoding to learn the
most likely token following its usage. We use
SpaCy's\footnote{\url{https://spacy.io}} nlp models to identify
proper nouns in each sentence, and replace the tokens.
\paragraph{NLTK Tokenization:}
We use NLTK's \texttt{tokenize} module to further process our text data.
We can use this module to split our strings into its component words,
punctuation, contrations, etc. We can use this to easily split words and train
on each token. We lower case all input before tokenizing.
\paragraph{SOS Tokens:}
During runtime, we encapsulate every sentence with two special tokens:
SOS and EOS. The Start of Sentence token (SOS) signals the start of a
sequence, and allows us to map our first real word to one that most likely
starts a sentence, given the current hidden state from the encoder
(and encoder outputs if attention is used).
\paragraph{EOS Tokens:}
We use the End of Sentence (EOS) token to signal the end of the sequence, i.e.
when to stop the decoding process. In the batched version, the EOS token
signals which tokens should be counted in the loss, while everthing after EOS
is masked (see Section \ref{sec:masked-loss}).
\paragraph{Data Split:}
We randomly shuffle and split the combined preprocessed dataset into three sets:
Train, Dev, and Test. We opted for a 87.5/10/2.5 split to reduce the appearance
of unkown tokens (UNK). This gives us a sizable training corpus to acutally
learn, and a reasonable valdiation set to measure BLEU scores and loss. The
small test set allows us to digest our results. Since the amount of Shakespeare
data is limited, and will not increase in size, we can have small test sets and
seek to overtrain the model.
\begin{figure}[ht]
    \centering
    \begin{tabular}{ |l|r| }
        \hline
        \textbf{File}
          & \textbf{Line Count}\\
        \hline
        train.snt.aligned & 9,069 \\ \hline
        dev.snt.aligned & 1,036 \\ \hline
        test.snt.aligned & 260 \\ \hline
        \textbf{Total} & \textbf{10,365} \\ \hline
    \end{tabular}

    \caption{Data Split for Shakespeare--English Corpus (Enotes)}
    \label{fig:data-lines-enotes}
\end{figure}
%------------------------------------------------
\section{Model Architecture}
\label{sec:models}
We developed several encoder-decoder style models with attention
as used in Cho et al. \cite{cho2014learning}
and Sutskever et al. \cite{sutskever2014sequence}.
\subsection{Overview}
Our approach is to use a state of the art neural MT system to
provide a reasonable translation of the source sequence to a target language.

NMT systems are comprised of two sub models: an Encoder and Decoder. The
encoder is know as the \emph{inference network} and encodes the input sequence
to provide context to the decoder, or \emph{generative network}. The decoder
then uses the encoder's output to generate tokens corresponding to words
in the target language. Attention mechanisms have shown to improve the
generative network, as detailed in Bahdanau et al. \cite{bahdanau2014neural},
and further work from Luong et al. \cite{luong2015effective}.

\subsection{Encoders}
The encoder, or \emph{inference network}, receives an input token sequence
$\vec{x} = \left[{x_1,\hdots, x_n}\right]$ of length $n$ and processes
this to create an output encoding. The result is a sequence
$\vec{h} = \left[{h_1, \cdots, h_{n}}\right]$ that
maps to the input sequence $\vec{x}$. The final hidden state is $h_n$.

Encoders share the same architecture except for the recurrent layer.
For each input word $x_j$, the encoder looks up the associated word embedding,
and runs the sequence through the recurrent layer. We define $W_e$ as the word
embedding, with each row corresponding to an input token.
\begin{equation}
  \vec{h} = \begin{cases}
    \operatorname{RNN} (W_e[\vec{x}]) \\
    \operatorname{GRU} (W_e[\vec{x}]) \\
    \operatorname{BiGRU} (W_e[\vec{x}]) \\
  \end{cases}
\end{equation}
The output of the recurrent layer is our encodings, and we propagate
the layer's last hidden state to the decoder. Theoretically, the hidden
state should encode all the important information from the input sequence
and pass it along to the decoder, but the
encodings can be used with attention to provide better models. If the encoder
type is bidirectional, then we return only the forward hidden state.
\begin{figure*}
    \centering
    \begin{tabular}{ |l|l|l|r| }
        \hline
        \textbf{Encoder}
          & \textbf{Decoder}
          & \textbf{Attention}
          & \textbf{Model Parameters}\\
        \hline
        RNN & RNN & None & 6,414,153 \\ \hline
        GRU & GRU & None & 6,940,489 \\ \hline
        BiGRU & GRU & None & 7,335,241 \\ \hline
        BiGRU & GRU & General & 7,859,530 \\ \hline
        BiGRU & GRU & Concat & 7,925,578 \\ \hline
    \end{tabular}

    \caption{Total Model Parameters (for Enote dataset)}
    \label{fig:data-lines-enotes}
\end{figure*}
\subsection{Decoders}
The decoder, or \emph{generative network}, receives the encoder outputs, the
model's hidden state, and the last input token. On the first pass of the
decoder, the hidden state is the encoder's final hidden state, and the
input token is SOS.

We can describe our decoders in two ways: with and without attention. For
our recurrent layers, we use the RNN and GRU layers. We cannot use a
Bidirectional GRU because we do not know the full decoded sequence
(and hence why we are decoding).
\begin{equation}
  \tau_i = \begin{cases}
    W_e[t_i] & \emph{No Attention} \\
    W_e[t_i] \,\, \Vert \,\, c_i & \emph{With Attention} \\
  \end{cases}
\end{equation}

In the case of attention, we apply one of the attention schemes
to the encoder output, given our current decoder's
hidden state. We concatenate the attention results, known as a context vector
$c_i$ with our input embedding for an input token $t_i$.
Without attention, we just use the input embedding and ignore the encoder
outputs.
\begin{equation}
  \mathbf{y} = W_d \cdot \begin{cases}
      \operatorname{RNN} (\tau_i, \, s_{i-1}) \\
      \operatorname{GRU} (\tau_i, \, s_{i-1}) \\
    \end{cases}
\end{equation}
We apply a recurrent layer to the attended tensor, using the hidden state
provided. After applying a linear layer with weights $W_d$ and applying a log
softmax, we output our result for evaluation, which is the log probability
distribution across the target vocabulary.
\begin{equation}
  \log \sigma(\mathbf{y}) = \log \frac{\exp(y_i)}{\sum_j \exp(y_j)}
\end{equation}
\subsection{Attention}
Attention mechanisms have been shown to improve sequence to sequence
translations from Bahdanau et al. \cite{bahdanau2014neural}, and further work
from Luong et al. \cite{luong2015effective} examines \emph{global} vs
\emph{local} approaches
to attention-based encoder-decoders. Common attention mechanisms are:
\begin{equation}
    score(s_{i-1}, h_j) =
    \begin{cases}
        v_a^T \cdot \tanh (W_a [s_{i-1} \| h_j]) & \text{\emph{concat}} \\
        s_{i-1}^T \cdot W_a \cdot h_j & \text{\emph{general}} \\
        s_{i-1}^T \cdot h_j & \text{\emph{dot}} \\
    \end{cases}
\end{equation}
where $s_{i-1}$ is the previous decoder hidden state, and $h_j$ is the $j$th
encoder output.
To compute scores, which are used to create attention weights, we apply a
softmax:
\begin{equation}
  a(s_{i-1}, h_j) = \frac{\exp(score(s_{i-1}, h_j))}{\sum_{j'}\exp(score(s_{i-1}, h_{j'}))}
\end{equation}
Using these scores, we create a context vector $c_i$, which is just the batch
matrix multiplication between our attention weights and the encoder outputs
(i.e. a weighted sum of the encoder outputs).
\begin{equation}
  c_i = \sum_{j'} a(s_{i-1}, h_{j'}) \cdot h_{j'}
\end{equation}
We will focus on general attention and concat attention in our
experiments.
\subsection{Teacher-Forcing}
In terms of training, an encoder-decoder system can either accept the target
token or the model's prediction as the next input during the decoding step.
Teacher forcing is when we use the target token instead of the model's
prediction and is
shown to be favored during initial training iterations but should be tapered
off to use the model's own predicitons. Continuous use of teacher forcing
without backing off to the model's own tokens will exhibit instability
in the translations as shown in Lamb et al. \cite{lamb2016professor}

During training, we randomly determine whether to feed in the true target
token or the model's decoded token to the decoder, and is configurable.
%------------------------------------------------
\section{Implementation}
\label{sec:implementation}
\paragraph{Batching:}
In order to improve training speed and prevent noisy gradients, we introduced
batching as recommended by Morishita et al.
\cite{morishita2017empirical}. We batch by the sorting the source sequences by
size in ascending order, and grouping sequences of equivalent sizes together.
We pad each target sequence with
the EOS token, up to the maximum target sequence length in the set. For our
dataset, at a maximum batch size of 128, we created 151 training batches and
65 dev batches. We do not batch test samplesm.
\paragraph{GPU/CPU:}
Standard training can be done on a CPU, but neural networks have been shown to
gain massive speedups. In order to improve training time,
we have allowed an optional parameter to use a gpu.
Our group has 3 GPUs (Morris: 1; Riley: 2). We distributed our tests across
multiple GPUs for faster experimentation.
\paragraph{Masked NLL Loss:}
\label{sec:masked-loss}
While we would normally use the \emph{Negative Log Likelihood Loss} (NLL) to
measure our error to backpropagate on, because of batching we must modify this
loss to mask tokens that are past the target length for a decoded sequence.
Hence we define a mask $m$ such that for a target token index $t_{ni}$ for the
$i$th decoded token of the $n$th sequence in the batch:
\begin{equation}
  m_{t_{ni}} = \begin{cases}
    1 & \text{if } i < target \text{ } length \\
    0 & \text{otherwise}
  \end{cases}
\end{equation}
We describe the masked loss for an input log probability vector $x_n$ and true
target token index $t_{ni}$ as:
\begin{equation}
  NLL(x_n, t_{ni}) = -x_{n, t_{ni}} \cdot m_{t_{ni}}
\end{equation}
We average our loss across the batch.
\paragraph{BLEU Scores:}
\label{sec:bleu}
We use the Bilingual Evaluation Understudy (BLEU) score as one of our criteria
for the quality of model translations. Outputs are in the range [0, 1], and a
BLEU score of 1 indicates a perfect translation to the reference texts.
However, we do not need not obtain a perfect score in order to have a good
translation model. A full description of how BLEU scores
are calculated from Papineni et al. \cite{papineni2002bleu} can be found in
Appendix \ref{app:bleu}

\begin{figure*}
    \centering
    \begin{tabular}{ |l|r|r| }
        \hline
        \textbf{Model}
          & \textbf{Source BLEU} & \textbf{Target BLEU} \\
        \hline
        RNN                             & 0.0000 & 0.0000 \\ \hline
        GRU                             & 0.0237 & 0.0238 \\ \hline
        BiGRU                           & 0.0297 & 0.0336 \\ \hline
        BiGRU + General                 & 0.0291 & 0.0333 \\ \hline
        BiGRU + General + 50\% TF       & 0.0264 & 0.0330 \\ \hline
        BiGRU + General + 100\% TF      & 0.0348 & 0.0395 \\ \hline
        BiGRU + Concat                  & 0.0953 & 0.0842 \\ \hline
        BiGRU + Concat + 50\% TF        & 0.1505 & 0.1349 \\ \hline
        BiGRU + Concat + 100\% TF       & 0.1153 & 0.1042 \\ \hline
        \textbf{Best Model}             & \textbf{0.1647} & \textbf{0.1450} \\ \hline
        \textbf{Original-Modern}  & \textbf{0.4465} & \textbf{0.4455} \\ \hline
    \end{tabular}
    \caption{Model BLEU Results for Experiments. Source BLEU compares results
    to the source (original) sentences. Target BLEU compares results to
    the target (modern) sentences. We aim for high Target BLEUs.}
    \label{fig:model-bleu}
\end{figure*}
%------------------------------------------------
\section{Results: Shakespeare-English}
\label{sec:results}
Our results demonstrate how different models and teaching methods affect the
final translations. In Figure \ref{fig:model-bleu} below, we can see the
final BLEU scores for our models. Our codebase can be found here: 
https://github.com/nextBillyonair/DLProject
\paragraph{RNN:}
The bare bones RNN had almost no capacity to learn and suffers from issues
such as the vansihing gradient problem and long term dependencies. Since many
of our sentences were long, this was problematic. The most distingiushing
feature of the RNN translations is the overuse of repeated punctuation.
\paragraph{GRU and Bidirectional GRU:}
The GRU showed better promise in acheiveing higher bleu scores, as seen in
Figure \ref{fig:model-bleu}, since GRUs migitgate the vanishing gradient
problem and have a higher capacity.
The Birdirectional GRU achieved slightly better results, most likely from
incorporating both a forward and backward context to the sequence. In addition,
the bidirectional has two sublayers, thus doubling the GRU capacity to learn.
\paragraph{Attention Mechanisms:}
Although our best results were from a model trained with concat attention, we
can see that the model trained with general attention actually worked well on
shorter sentences (but concat still outperformed general).
An explanation for this is that Luong et al.
\cite{luong2015effective} attention mechanisms work best in a local setting.
This means that either the sentnece is short, or the model only looks at a
small window of the full sequence. It is reasonable to assume that since we
looked at the global context of our source encodings, the general attention
failed whereas the Bahdanau \cite{bahdanau2014neural} concat attention
performed better overall.
\paragraph{Teacher Forcing:}
The 'best model' score came from our BiGRU architecture with concat attention
and 100\% teacher forcing. Looking at the scores, we can actually see that our
model trained with 50\% teacher forcing did better than the one with 100\%, but
our final best results were from the one with 100\% and after many iterations
with random initializations and keeping the trained model that worked best.
This is important to note since it demonstrates how randomness in
initializations can greatly affect the final outcome. If we had more time and
resources, we would train a model that starts with 100\% teacher forcing and
slowly taper down as training progresses.
\paragraph{Best Model:}
Our best model, as selected by test BLEU score, was the BiGRU architecture
with concat attention and 100\% teacher forcing, after training for 500
iterations. This model acheive a target BLEU score of $0.1450$. The next best
model was the BiGRU with concat attention and 50\% teacher forcing, with target
BLEU score of $0.1349$.
\paragraph{Final Analysis:}
This shows that attention is necessary for good results, and that teacher
forcing helps the model achieve better translations by feeding in the true
target tokens. However, these models can be trained longer, and the effects
of random initialization is not clear. In addition, more training dynamics can
be incorporated to provide deeper insight into this problem. See Appendix
\ref{app:model-results} for sample translations.
%------------------------------------------------
\section{Key Takeaways}
\label{sec:key-takeaway}
\paragraph{General vs Local Attention:}
\label{sec:key-attention}
Our attention models use the \emph{global} approach which attends to all the
source embeddings at once. However, Luong et al. \cite{luong2015effective}
showed that by using a \emph{local} approach that attends to only a subset
of the encodings, a nerual encoder-decoder system could make effective gains in
BLEU scores. In our experiments, both attention models used the \emph{global}
approach, and it is clear why general failed to do well comapred to the
Bahdanau concat attention. Luong states that \emph{global}
attention fails on long sequences whereas a local approach is more effective.
The \emph{local} approach takes
source embeddings within a window, computes the attention, and weights the
attention probabilities by a guassian distribution.
\paragraph{Teacher Forcing Impact:}
Teacher Forcing was required to make effective gains in our attention models.
However, our results are inconclusive on how much teacher forcing should be
applied. According to Lamb et al. \cite{lamb2016professor}, teacher forcing
acts as a regularizer for these systems. In addition, Bengio et al.
\cite{DBLP:journals/corr/BengioVJS15} describes an annealing
process, changing the task from an easy one with target tokens, to a more
realistic approach of feeding in the model's prediction.
\paragraph{Random Initialization:}
As our results show, different initializations can lead to different BLEU
scores, and different generalization performance. We saw this between our
concat models that used teacher forcing.
\paragraph{GPU vs CPU Training Times:}
Training on a GPU yielded significant performance increases, allowing us to
train multiple models quickly. Our model with the largest capacity took 11-13
minutes on a standard CPU, but only 111 seconds on a GPU for 2 full epochs.
%------------------------------------------------
\section{Conclusion \& Future Work}
\label{sec:conclusion}
If given more time, obvious extensions to our existing experiments would
included:
\paragraph{Batch Annealing:}
We wanted to study the effects of adaptive batch sizes,
where training would
begin with a small batch size and progressively increase to very large sizes.
This would allow for stochastic updates in the begining before approximating
full gradients in later iterations. We think this would have improved
training and provided better results.
\paragraph{English to Shakespeare Modeling:} We wanted to also train a reverse
model for English to Shakespeare that applies
a translation between English sentences and turns them into Shakespeare sentences.
This is a weak form of style transfer masked as a translation problem.
However, more work needs to be done before we find the best architecture.
\paragraph{Local Attention with Window:} As discussed in Section
\ref{sec:key-attention}, incorporating a window to allow
for \emph{local} attention over \emph{global} could results in better
models. General might perform better in a \emph{local} context over a
\emph{global} approach.
\paragraph{Data Processing:} We may want to look into other processing pipelines,
such as the ones found in SMT platforms Moses or GIZA++. In addition,
we may want to look into BPE tokenization to see if it reduces our vocabulary
size. BPE tokenization segments text into subword units, and has been found
to significantly reduce vocabulary sizes in some languages. More time could be
spent on refining our current dataset, and procuring more data to train on.
Finally, we should sample test sentences such that no unkown words are in it,
improving test results.
%------------------------------------------------
\section{Advice to Future Students}
\label{sec:advice}
It's important for students to understand that most ML problems
rely on vast amounts of data. Now, deep learning systems have been shown in
practice to handle raw data effectively, but time should still be spent
on refining the data. We had this issue between two corpora that drastically
differed in reordering of text, making one more difficult to learn.

Additionally, strong foundations of linear algebra, multivariate calculus,
optimization and probability are required to understand how these models
are constructed and trained. Multivariate calculus and optimization is
fundamental to backpropagation and gradient descent.
Probability gives us maximum likelihood estimation, the basis for
most loss functions, and influences our model's approaches to
predicition. Linear Alegbra is essential to expressing complex equations into
simple statements, and understanding the fundamentals of neural networks
and how they transform inputs.

The more advanced models such as VAEs, RBMs, and
generative models become accessable with the prerequisite knowledge.
Therefore, students should be taught
these subjects with respect to neural networks as early as possible. However,
the instructors should attempt to teach neural networks in a non-standard way,
as simple classification and regression results in rigid thinking oh how
models can be designed, and discussion on how models can be trained in
non-standard ways, such as NMT models or VAEs, would be great.
%------------------------------------------------

% References
\bibliographystyle{abbrv}
\bibliography{final_report}

%------------------------------------------------

\clearpage
\appendix
\onecolumn
\begin{myparindent}{0pt}
\section{Additional Information}
\subsection{RNN}
The simplest way to process sequences of inputs is to use recurrent layers,
which accept an input and previous hidden state to update its output.
Our baseline models incorporate the \emph{Elman RNN}, the simplest recurrent
layer.
\begin{equation}
  \label{eq:rnn}
  h_t = \tanh(W_{ih} x_t + b_{ih}  +  W_{hh} h_{t-1} + b_{hh})
\end{equation}
However, the layer is prone to the vanishing gradient problem
on large sequences. In addition, \emph{Elman RNNs} have the lowest capacity
to learn.
\subsection{GRU}
Improvements have been made to the baseline RNN to increase capacity and
improve gradient flow over long sequences. \emph{Long Short-Term Memory} (LSTM)
and \emph{Gated Recurrent Units} (GRU) are two of the most popular improvements
to recurrent networks. Both solve the vanishing gradient problem found in
traditional RNN layers, and improve a model's capacity to learn.
\begin{equation}
  \label{eq:gru}
  \begin{split}\begin{array}{ll}
    r_t = \sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{t-1} + b_{hr}) \\
    z_t = \sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{t-1} + b_{hz}) \\
    n_t = \tanh(W_{in} x_t + b_{in} + r_t \circ (W_{hn} h_{t-1}+ b_{hn})) \\
    h_t = (1 - z_t) \circ n_t + z_t \circ h_{t-1} \\
  \end{array}\end{split}
\end{equation}
For our models, we use GRUs, as they have been shown to perform as well as
LSTMs but with less parameters.
\subsection{Bidirectional GRU}
GRUs, and RNNs in general, only process sequences in a forward direction.
However, encoding both the preceding and succeeding input would be beneficial
to learning the context of a word in its local vicinity. Bidirectional layers
allow models to encode inputs in both the forward and reverse direction. This
is a simple, yet poweful, extension to uni-directional recurrent layers.
\begin{equation}
  \label{eq:bidirectional}
  \begin{split}
    \begin{array}{ll}
      \overrightarrow{h_f} = \operatorname{GRU}(\overrightarrow{input})\\
      \\
      \overleftarrow{h_b} = \operatorname{GRU}(\overleftarrow{input})\\
      \\
      h_o = \overrightarrow{h_f} \,\,\Vert \,\, \overrightarrow{h_b}\\
    \end{array}
  \end{split}
\end{equation}
Bidirectional layers run two separate recurrent layers, one on the forward input
sequence; the other on the reversed input. Thus we get two output tensors, one
for each direction. We reverse the backward tensor, and stack the two output
tensors together. Thus our final tensor encodes the forward and backward
context.
\subsection{BLEU Scores}
\label{app:bleu}
We use the Bilingual Evaluation Understudy (BLEU) score as one of our criteria
for the quality of model translations. Outputs are in the range [0, 1], and a
BLEU score of 1 indicates a perfect translation to the reference texts.
We do not need not obtain a perfect score in order to have a good translation
model. BLEU scores are calculated for individual sentences and averaged across
the whole corpus. Intelligibility and grammatical correctness are not taken
into account when calculating BLEU scores. According to the original paper by
Papineni et al. \cite{papineni2002bleu}, BLEU scores are computed as follows:

\begin{equation}
  \operatorname{BLEU} = \operatorname{BP} \cdot \exp \left( \sum_{n=1}^N w_n \log p_n \right)
\end{equation}

\noindent
where $p_n$ is the geometric average of the modified $n$-gram precisions
and $\operatorname{BP}$ is the brevity penalty, explained below.
(\cite{papineni2002bleu} uses $N=4$, uniform weights $w_n = \frac{1}{N}$.)
The brevity penalty $\operatorname{BP}$ is used for translations that are
too short compared to the reference. BP is defined as:

\begin{equation}
  \operatorname{BP} = \begin{cases}
    1 & \text{if } c > r \\
    e^{(1-r/c)} & \text{if } c \leq r
  \end{cases}
\end{equation}
\noindent
with effective reference corpus length $r$ and candidate translation length $c$.
BLEU scores are non-differentiable, and therefore we cannot directly optimize
on this metric, but can use it to enhance our understanding regarding the
quaity of our models given the data.

\section{Model Results}
\label{app:model-results}
\begin{figure}[ht!]
    \centering
    \begin{tabular}{ |l|r| }
        \hline
        \textbf{Model}
          & \textbf{Sentence} \\
        \hline
        \textbf{Source} & \textbf{double , double , toil and trouble ; fire burn and cauldron bubble .} \\ \hline
        RNN & damned as you are , you have cast the spell , the [followed by 490 commas] \\ \hline
        GRU & double , double , toil and ; ; and burn burn burn burn caldron bubble . \\ \hline
        BiGRU & double , double , toil and ; ; burn , , , and caldron bubble . . \\ \hline
        BiGRU + General & double , double , toil , and ; , , burn , , bubble bubble . \\ \hline
        BiGRU + General + 50\% TF & double , double , toil and trouble ; fire , burn ; and caldron , bubble . \\ \hline
        BiGRU + General + 100\% TF & double , double , toil and trouble ; fire , burn ; and caldron , bubble . \\ \hline
        BiGRU + Concat & double , double , toil and trouble ; , fire burn ; and caldron bubble bubble . \\ \hline
        BiGRU + Concat + 50\% TF & double , double , toil and trouble ; fire , burn ; and caldron , bubble . \\ \hline
        BiGRU + Concat + 100\% TF & double , double , toil and trouble ; fire , burn ; and caldron , bubble . \\ \hline
        \textbf{Best Model} & \textbf{double , double , toil and trouble ; fire , burn ; and caldron , bubble .} \\ \hline
        \textbf{Target} & \textbf{double , double , toil and trouble ; fire , burn ; and caldron , bubble .} \\ \hline
    \end{tabular}
    \caption{Model Results for Selected Source-Target Pair: \#1}
    \label{fig:model-results-1}
\end{figure}

\begin{figure}[ht!]
    \centering
    \begin{tabular}{ |l|r| }
        \hline
        \textbf{Model}
          & \textbf{Sentence} \\
        \hline
        \textbf{Source} & \textbf{tonight , my lord ?} \\ \hline
        RNN & what you you , \\ \hline
        GRU & how , my lord ? \\ \hline
        BiGRU & my , my lord ? \\ \hline
        BiGRU + General & , my lord lord ? \\ \hline
        BiGRU + General + 50\% TF & my lord lord ? \\ \hline
        BiGRU + General + 100\% TF & brother , my lord ? \\ \hline
        BiGRU + Concat & tonight , my lord ? \\ \hline
        BiGRU + Concat + 50\% TF & three , , my lord ? \\ \hline
        BiGRU + Concat + 100\% TF & tonight , my lord ? \\ \hline
        \textbf{Best Model} & \textbf{tonight , my lord ?} \\ \hline
        \textbf{Target} & \textbf{tonight , my lord ?} \\ \hline
    \end{tabular}
    \caption{Model Results for Selected Source-Target Pair: \#2}
    \label{fig:model-results-2}
\end{figure}

\begin{figure}[ht!]
    \centering
    \begin{tabular}{ |l|r| }
        \hline
        \textbf{Model}
          & \textbf{Sentence} \\
        \hline
        \textbf{Source} & \textbf{what say you , propn ?} \\ \hline
        RNN & what , you , \\ \hline
        GRU & what do you , propn propn ? \\ \hline
        BiGRU & what do you say , propn ? \\ \hline
        BiGRU + General & what do you say , propn ? \\ \hline
        BiGRU + General + 50\% TF & what do you say , propn ? \\ \hline
        BiGRU + General + 100\% TF & what do you say , propn ? \\ \hline
        BiGRU + Concat & what did you say , propn ? \\ \hline
        BiGRU + Concat + 50\% TF & what do you say , propn ? \\ \hline
        BiGRU + Concat + 100\% TF & what do you say , propn propn ? \\ \hline
        \textbf{Best Model} & \textbf{what do you say , propn ?} \\ \hline
        \textbf{Target} & \textbf{what do you say , propn ?} \\ \hline
    \end{tabular}
    \caption{Model Results for Selected Source-Target Pair: \#3}
    \label{fig:model-results-3}
\end{figure}

\begin{figure}[ht!]
    \centering
    \begin{tabular}{ |l|r| }
        \hline
        \textbf{Model}
          & \textbf{Sentence} \\
        \hline
        \textbf{Source} & \textbf{propn thou not see her paddle with the palm of his hand ?} \\ \hline
        RNN & do n ' t \\ \hline
        GRU & will n ' t storms ; his his his the ? ? \\ \hline
        BiGRU & propn ' n ' t with you like the the the the hand hand ? \\ \hline
        BiGRU + General & does ’ ’ t you ’ t his his his his his all ? \\ \hline
        BiGRU + General + 50\% TF & propn , remember you in the hand of night of great hand ? \\ \hline
        BiGRU + General + 100\% TF & might do n ’ t see the his dying hand , might see about the ? \\ \hline
        BiGRU + Concat & do you ' t that you speak with the with such of of hand hand ? \\ \hline
        BiGRU + Concat + 50\% TF & have n ' t see her with the same of of his his hand ? \\ \hline
        BiGRU + Concat + 100\% TF & do n ' t you see her for the scripture of his body ? \\ \hline
        \textbf{Best Model} & \textbf{did n ' t go see her close with the same of his hand ?} \\ \hline
        \textbf{Target} & \textbf{did n ’ t you see her play with the palm of his hand ?} \\ \hline
    \end{tabular}
    \caption{Model Results for Selected Source-Target Pair: \#4}
    \label{fig:model-results-4}
\end{figure}

\begin{figure}[ht!]
    \centering
    \begin{tabular}{ |l|r| }
        \hline
        \textbf{Model}
          & \textbf{Sentence} \\
        \hline
        \textbf{Source} & \textbf{some wine , ho !} \\ \hline
        RNN & what , ho ! \\ \hline
        GRU & what , , , ! \\ \hline
        BiGRU & and , ho ! \\ \hline
        BiGRU + General & give , oh ! ! \\ \hline
        BiGRU + General + 50\% TF & oh , oh , hello ! \\ \hline
        BiGRU + General + 100\% TF & some three , a blessing in like a ! \\ \hline
        BiGRU + Concat & some ! , ho ! \\ \hline
        BiGRU + Concat + 50\% TF & let me fight ! \\ \hline
        BiGRU + Concat + 100\% TF & some whiskey , hello ! \\ \hline
        \textbf{Best Model} & \textbf{some wine , ho !} \\ \hline
        \textbf{Target} & \textbf{some wine , ho !} \\ \hline
    \end{tabular}
    \caption{Model Results for Selected Source-Target Pair: \#5}
    \label{fig:model-results-5}
\end{figure}

\begin{figure}[ht!]
    \centering
    \begin{tabular}{ |l|r| }
        \hline
        \textbf{Model}
          & \textbf{Sentence} \\
        \hline
        \textbf{Source} & \textbf{down , strumpet !} \\ \hline
        RNN & oh , \\ \hline
        GRU & hey , hey prostitute \\ \hline
        BiGRU & prostitute , ! ! \\ \hline
        BiGRU + General & no , ! \\ \hline
        BiGRU + General + 50\% TF & propn , ! \\ \hline
        BiGRU + General + 100\% TF & prostitute ! \\ \hline
        BiGRU + Concat & down , , prostitute ! \\ \hline
        BiGRU + Concat + 50\% TF & peace , prostitute ! \\ \hline
        BiGRU + Concat + 100\% TF & down , prostitute ! \\ \hline
        \textbf{Best Model} & \textbf{down , prostitute !} \\ \hline
        \textbf{Target} & \textbf{down , prostitute !} \\ \hline
    \end{tabular}
    \caption{Model Results for Selected Source-Target Pair: \#6}
    \label{fig:model-results-6}
\end{figure}

\begin{figure}[ht!]
    \centering
    \begin{tabular}{ |l|r| }
        \hline
        \textbf{Model}
          & \textbf{Sentence} \\
        \hline
        \textbf{Source} & \textbf{let each man render me his bloody hand .} \\ \hline
        RNN & and , , the , , , , , \\ \hline
        GRU & his me his him him a , him revenge . . \\ \hline
        BiGRU & then , let his ; me ' m me me . \\ \hline
        BiGRU + General & let me , where where to his ’ . . \\ \hline
        BiGRU + General + 50\% TF & let me leave his hand away . \\ \hline
        BiGRU + General + 100\% TF & let me me me , good hand hand , but hand hand hand me . \\ \hline
        BiGRU + Concat & let give they give me me his hand hand . \\ \hline
        BiGRU + Concat + 50\% TF & let the man from give me his honorable . \\ \hline
        BiGRU + Concat + 100\% TF & let go get saying any man . \\ \hline
        \textbf{Best Model} & \textbf{let me shake his beard hand .} \\ \hline
        \textbf{Target} & \textbf{let each man give me his bloody hand .} \\ \hline
    \end{tabular}
    \caption{Model Results for Selected Source-Target Pair: \#7}
    \label{fig:model-results-7}
\end{figure}

% \begin{figure}[ht!]
%     \centering
%     \begin{tabular}{ |l|r|r| }
%         \hline
%         \textbf{Source Shakespeare Sentence}
%           & \textbf{General Attention Output} & \textbf{Target} \\
%         \hline
%         i am much bound to you .          & i am to you .
%         & thank you so much .             \\ \hline
%         o propn , they fight !            & o propn , listen !
%         & o lord , they ’ re fighting !   \\ \hline
%         do you like this haste ?          & honestly must hurry ?
%         & do you like this speed ?        \\ \hline
%         wilt thou provoke me ?            & you wilt make me ?
%         & will you still provoke me ?     \\ \hline
%         give me your neaf , propn propn . & let me your hand propn .
%         & give me your hand , propn propn . \\ \hline
%     \end{tabular}
%     \caption{ This demonstrates that general attention can work well on shorter sentences.}
%     \label{fig:short-results}
% \end{figure}
\newpage
\section{Model Architecture Diagrams}

\begin{figure}[ht!]
  \centering
  \begin{tikzpicture}

    % Define nodes
    \node[rectangle] (encoder) {Encoder};
    \node[rectangle, below=2cm of encoder] (decoder) {Decoder};
    \node[rectangle, above=0.75cm of encoder, xshift=-1cm] (s) {$source$};
    \node[rectangle, above=0.75cm of encoder, xshift=1cm] (hidden1) {$None$};
    \node[rectangle, below=0.75cm of encoder, xshift=-1cm] (out1) {$encodings$};
    \node[rectangle, below=0.75cm of encoder, xshift=1cm] (hidden2) {$hidden$};
    \node[rectangle, left=0.75cm of decoder] (t) {$token$};
    \node[rectangle, below=0.75cm of decoder, xshift=-1cm] (out2) {$new$ $token$};
    % \node[rectangle, below=0.75cm of decoder, xshift=1cm] (hidden3) {$hidden$};

    % Connect the nodes
    \edge {s, hidden1} {encoder};
    \edge {encoder} {out1, hidden2};
    \edge {out1, t} {decoder};
    \edge {decoder} {out2};
    \edge {out2} {t};
    \path [->] (decoder) edge [bend right=15] node {} (hidden2);
    \path [->] (hidden2) edge [bend right=15] node {} (decoder);

  \end{tikzpicture}
  \caption{Model Architecture Overview for Encoder-Decoder.}
  \label{fig:encoder-decoder}
\end{figure}

\begin{figure}[ht!]
  \centering
  \begin{tikzpicture}

    % Define nodes
    \node[rectangle] (rnn) {RNN/GRU};
    \node[rectangle, below=0.75cm of rnn, xshift=-1cm] (out) {$output$};
    \node[rectangle, below=0.75cm of rnn, xshift=1cm] (hidden2) {$hidden$};
    \node[rectangle, above=0.75cm of rnn, xshift=-1cm] (embed) {Embed};
    \node[rectangle, above=0.75cm of embed] (input) {$source$};
    \node[rectangle, right=0.75cm of input] (hidden1) {$hidden$};

    % Connect the nodes
    \edge {input} {embed};
    \edge {embed, hidden1} {rnn};
    \edge {rnn} {out, hidden2};

  \end{tikzpicture}
  \caption{Model Architecture for Encoder.}
  \label{fig:encoder}
\end{figure}

\begin{figure}[ht!]
  \centering
  \begin{tikzpicture}

    % Define nodes
    \node[rectangle] (rnn) {RNN/GRU};
    \node[rectangle, above=0.75cm of rnn] (embed) {Embed};
    \node[rectangle, above=0.75 of embed] (input) {$input$};
    \node[rectangle, right=1cm of input] (hidden1) {$hidden$};
    \node[rectangle, below=0.75cm of rnn] (linear) {Linear};
    \node[rectangle, below=0.75cm of linear] (logs) {Log Softmax};
    \node[rectangle, below=0.75cm of logs] (out) {$output$};
    \node[rectangle, right=1.25cm of out] (hidden2) {$hidden$};

    % Connect the nodes
    \edge {input} {embed};
    \edge {embed, hidden1} {rnn};
    \edge {rnn} {linear, hidden2};
    \edge {linear} {logs};
    \edge {logs} {out};

  \end{tikzpicture}
  \caption{Decoder with No Attention.}
  \label{fig:decoder-no-attn}
\end{figure}

\begin{figure}[ht!]
  \centering
  \begin{tikzpicture}

    % Define nodes
    \node[rectangle] (rnn) {RNN/GRU};
    \node[rectangle, above=0.75 of rnn, xshift=-1cm] (cat) {Concat};
    \node[rectangle, above=0.75cm of cat] (attn) {Attn};
    \node[rectangle, left=0.75cm of attn] (embed) {Embed};
    \node[rectangle, above=0.75 of embed] (input) {$input$};
    \node[rectangle, above=0.75cm of attn] (encoder) {$encoding$};
    \node[rectangle, right=1cm of encoder] (hidden1) {$hidden$};
    \node[rectangle, below=0.75cm of rnn] (linear) {Linear};
    \node[rectangle, below=0.75cm of linear] (logs) {Log Softmax};
    \node[rectangle, below=0.75cm of logs] (out) {$output$};
    \node[rectangle, right=1cm of out] (hidden2) {$hidden$};

    % Connect the nodes
    \edge {input} {embed};
    \edge {encoder, hidden1} {attn};
    \edge {attn, embed} {cat};
    \edge {cat, hidden1} {rnn};
    \edge {rnn} {linear, hidden2};
    \edge {linear} {logs};
    \edge {logs} {out};

  \end{tikzpicture}
  \caption{Decoder with Attention.}
  \label{fig:decoder-attn}
\end{figure}

\begin{figure}[ht!]
  \centering
  \begin{tikzpicture}

    % Define nodes
    \node[rectangle] (out) {$output$};
    \node[rectangle, above=0.75cm of out] (bmm) {BMM};
    \node[rectangle, above=0.75cm of bmm, xshift=-1cm] (soft) {Softmax};
    \node[rectangle, above=0.75 of soft] (lin2) {Linear};
    \node[rectangle, above=0.75 of lin2] (tanh) {$\tanh$};
    \node[rectangle, above=0.75 of tanh] (lin1) {Linear};
    \node[rectangle, above=0.75 of lin1] (cat) {Concat};
    \node[rectangle, above=0.75 of cat, xshift=-1cm] (hidden) {$hidden$};
    \node[rectangle, above=0.75 of cat, xshift=1cm] (encodings) {$encodings$};

    % Connect the nodes
    \edge {encodings, hidden} {cat};
    \edge {cat} {lin1};
    \edge {lin1} {tanh};
    \edge {tanh} {lin2};
    \edge {lin2} {soft};
    \edge {soft, encodings} {bmm};
    \edge {bmm} {out};

  \end{tikzpicture}
  \caption{Concat (Bahdanau) Attention Layer.}
  \label{fig:concat-attn}
\end{figure}

\begin{figure}[ht!]
  \centering
  \begin{tikzpicture}

    % Define nodes
    \node[rectangle] (out) {$output$};
    \node[rectangle, above=0.75cm of out] (bmm) {BMM};
    \node[rectangle, above=0.75cm of bmm, xshift=-1cm] (soft) {Softmax};
    \node[rectangle, above=0.75 of soft] (bilinear) {Bilinear};
    \node[rectangle, above=0.75 of bilinear, xshift=-1cm] (hidden) {$hidden$};
    \node[rectangle, above=0.75 of bilinear, xshift=1cm] (encodings) {$encodings$};

    % Connect the nodes
    \edge {encodings, hidden} {bilinear};
    \edge {lin2} {soft};
    \edge {soft, encodings} {bmm};
    \edge {bmm} {out};

  \end{tikzpicture}
  \caption{General Attention Layer.}
  \label{fig:general-attn}
\end{figure}

\end{myparindent}
\end{document}
