%------------------------------------------------------------------------------
%   PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%------------------------------------------------------------------------------

\documentclass[twoside,twocolumn]{article}

%\usepackage[sc]{mathpazo} % Use the Palatino font
%\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
%\linespread{1.05} % Line spacing - Palatino needs more space between lines
%\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage[english]{babel} % Language hyphenation and typographical rules

% Document margins
\usepackage[hmarginratio=1:1,top=32mm,left=20mm,right=20mm,columnsep=20pt]{geometry}
% Custom captions under/above floats in tables or figures
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption}
\usepackage{booktabs} % Horizontal rules in tables

\usepackage{enumitem} % Customized lists
\setlist[itemize]{noitemsep} % Make itemize lists more compact
\usepackage{textcomp}

% Allows abstract customization
\usepackage{abstract}
% Set the "Abstract" text to bold
\renewcommand{\abstractnamefont}{\normalfont\bfseries}
% Set the abstract itself to small italic text
\renewcommand{\abstracttextfont}{\normalfont\small\itshape}

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{\thetitle}
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\usepackage{titling} % Customizing the title section

\usepackage{hyperref} % For hyperlinks in the PDF
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{tikz}
\usetikzlibrary{bayesnet, arrows, positioning, fit, arrows.meta, shapes}

\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{graphicx}


\captionsetup[figure]{labelfont={bf},textfont=normalfont}

%------------------------------------------------------------------------------
%   TITLE SECTION
%------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip} % Move the title up

\pretitle{\begin{center}\Huge\bfseries} % Article title formatting
\posttitle{\end{center}} % Article title closing formatting

\title{DL Project Final Report:\\
       Shakespeare - English Sequence to Sequence Modeling}
\author{%
\textsc{Morris Kraicer} \\[1ex]
\normalsize Johns Hopkins University \\
\normalsize \href{mailto:mkraice1@jhu.edu}{mkraice1@jhu.edu}
 \and
 \textsc{Riley Scott} \\[1ex]
\normalsize Johns Hopkins University \\
\normalsize \href{mailto:rscott39@jhu.edu}{rscott39@jhu.edu}
 \and
  \textsc{William Watson} \\[1ex]
\normalsize Johns Hopkins University \\
\normalsize \href{mailto:billwatson@jhu.edu}{billwatson@jhu.edu}
}

\date{}%\today} % Leave empty to omit a date
% \renewcommand{\maketitlehookd}{%

% }

%------------------------------------------------------------------------------

\begin{document}

% Print the title
\maketitle

%------------------------------------------------------------------------------
%   ARTICLE CONTENTS
%------------------------------------------------------------------------------

%------------------------------------------------

\begin{abstract}
% NN and NMT + attention + LSTM
\noindent
We present our attempt to create an English-Shakespeare Sequence to Sequence
Model, and its application to create a pseudo supervised style transfer system
for text. We experimented with different Encoder-Decoder Architectures with and
without Attention Mechanisms. We discuss our data procurement and processing
to help facilitate learning, and present our experimental results.
\end{abstract}

%------------------------------------------------
\section{Introduction}
We applied a suite of Encoder-Decoder models with varying attention
mechanisms (and lack of attention mechanisms) along with teacher forcing rates
to a corpus of Modern English - Original Shakespeare data. Discussion of our
data is in Section \ref{sec:data},
our processing methods are described in Section \ref{sec:preprocess}.
We used our neural machine translation systems to
apply a supervised style transfer between the two forms of writing.
More specifically, we tested models currently
used in decoding languages (Section \ref{sec:models}) and how they can
be used in this capacity. We provide results for several model
combinations and discuss the successes and failures of each system
(Section \ref{sec:results}). We also describe important implementation details
in Section \ref{sec:implementation}. Our final results are described in
Section \ref{sec:style-transfer} for our style transfer system.
Diagrams for our models can be found in the appendix.
%------------------------------------------------
\section{Data}
\label{sec:data}
Most of our data came from previous alignment work
by Xu et al.\cite{xu2012paraphrasing}, in which
two aligned corpora were procured from Sparknotes and Enotes. Both datasets,
deriving from different websites, differed drastically. Sparknotes was more
liberal in its translations, opting for reordering that differed drastically
from the original plays, and was harder to learn. Enotes was a smaller corpus
of 8 plays, but had less reordering, and was easier to train on.
The data was aligned, and not all of the original lines
from the plays are included (i.e. the sentences could not be aligned properly).
\footnote{\url{https://github.com/cocoxu/Shakespeare}}

\begin{figure}[ht]
    \centering
    \begin{tabular}{ |l|r| }
        \hline
        \textbf{Play}
          & \textbf{Line Count} \\
        \hline
        Hamlet & 2,010 \\ \hline
        Julius Caesar & 1,201 \\ \hline
        Macbeth & 1,085 \\ \hline
        Merchant of Venice & 831 \\ \hline
        Midsummer Nights Dream & 833 \\ \hline
        Othello & 1,893 \\ \hline
        Romeo and Juliet & 1,743 \\ \hline
        Tempest & 769 \\ \hline
        \textbf{Total} & \textbf{10,365} \\ \hline
    \end{tabular}

    \caption{Line Counts for Shakespeare--English Corpus per Play (Enotes)}
    \label{fig:data-lines-enotes}
\end{figure}

The vocabulary sizes are 9,004 source (original) words and
7,497 target (modern) words for the Sparknotes set. We had a total of 10,365
lines of Shakespeare plays to train on. There is a total word count of 233,282
words for the entire corpus (unprocessed). Sample data pairs (processed) are
provided in Figure \ref{fig:sample-pairs}.
\begin{figure*}[ht]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{ |c|l|l| }
      \hline
          $\#$
          & \textbf{Original Sentence}
          & \textbf{Modern Sentence}\\
      \hline
      1 & take me with you , take me with you , wife . & catch me , catch me , wife . \\
      \hline
      2 & that 's fouler . & that 's even more evil . \\
      \hline
      3 & perchance till after propn ' wedding - day . & maybe until after propn ' wedding - day . \\
      \hline
      4 & propn truly knows that thou art false as hell . & propn truly knows that you are as false as hell . \\
      \hline
      5 & then weep no more . & so stop crying . \\
      \hline
      6 & of propn , a n't please your mastership . & of propn , if it pleases you , sir . \\
      \hline
      7 & how ill this taper burns ! & how badly this candle burns ! \\
      \hline
      8 & the charm 's wound up . & the charm â€™ s going to bring things to a head . \\
      \hline
      9 & before the time be out ? & before the debt is paid ? \\
      \hline
      10 & but soft ! & quiet ! \\
      \hline
      \end{tabular}}
  \caption{Sample Original-Modern Sentence Pairs, Processed}
  \label{fig:sample-pairs}
\end{figure*}
%------------------------------------------------
\section{Preprocessing}
\label{sec:preprocess}
We applied several techniques to improve the quality of our data and reduce the
vocabulary size to train on.
\subsection{Proper Nouns}
Proper nouns are unique in both corpus, and have direct translations. In order
to reduce vocabulary size and aggregate the learning of all proper nouns,
we replace all proper nouns with the following token: propn. Thus our model
should learn to map propn to propn, and can utilize the encoding to learn the
most likely token following its usage. We use
SpaCy's\footnote{\url{https://spacy.io}} nlp models to identify
proper nouns in each sentence, and replace the tokens.
\subsection{NLTK Tokenization}
We use NLTK's \texttt{tokenize} module to further process our text data.
We can use this module to split our strings into its component words,
punctuation, contrations, etc.

For instance, the modern sentence from Hamlet
\emph{there's rue for you, and here's some for me.} will be tokenized into:
\emph{there 's rue for you , and here 's some for me .}

Additionally, contrations are split: \emph{isn't he honest?} tokenizes to
\emph{is n \textquotesingle} \emph{t he honest ?}

We can use this to easily split words and train on each token. We lower case
all input before tokenizing.
\subsection{SOS/EOS Tokens}
During runtime, we encapsulate every sentence with two special tokens:
SOS and EOS. The Start of Sentence token (SOS) signals the start of a
sequence, and allows us to map our first real word to one that most likely
starts a sentence, given the current hidden state from the encoder
(and encoder outputs if attention is used).

We use the End of Sentence (EOS) token to signal the end of the sequence, i.e.
when to stop the decoding process. In the batched version, the EOS token
signals which tokens should be counted in the loss, while everthing after EOS
is masked (see Section \ref{sec:masked-loss}).
\subsection{Data Split}
We randomly shuffle and split the combined preprocessed dataset into three sets:
Train, Dev, and Test. We opted for a 87.5/10/2.5 split to reduce the appearance
of unkown tokens (UNK). This gives us a sizable training corpus to acutally
learn, and a reasonable valdiation set to measure BLEU scores and loss. The
small test set allows us to digest our results. Since the amount of Shakespeare
data is limited, and will not increase in size, we can have small test sets and
seek to overtrain the model.
\begin{figure}[ht]
    \centering
    \begin{tabular}{ |l|r| }
        \hline
        \textbf{File}
          & \textbf{Line Count}\\
        \hline
        train.snt.aligned & 9,069 \\ \hline
        dev.snt.aligned & 1,036 \\ \hline
        test.snt.aligned & 260 \\ \hline
        \textbf{Total} & \textbf{10,365} \\ \hline
    \end{tabular}

    \caption{Data Split for Shakespeare--English Corpus (Enotes)}
    \label{fig:data-lines-enotes}
\end{figure}
%------------------------------------------------
\section{Model Architecture}
\label{sec:models}
We developed several models to create our pseudo style-transfer system,
incorporating context, attention, and training methods. It encorporates
an encoder-decoder style model as used in Cho et al. \cite{cho2014learning}
and Sutskever et al. \cite{sutskever2014sequence}.
\subsection{Overview}
Our idea is inspired by reducing our style-transfer system to a translation
problem. Hence our approach to provide state of the art neural MT systems to
provide a reasonable transfer of the target styling for a given source sequence.

NMT systems are comprised of two sub models: an Encoder and Decoder. The
encoder is know as the \emph{inference network} and encodes the input sequence
to provide context to the decoder, or \emph{generative network}. The decoder
then uses the encoder's output to generate tokens corresponding to words
in the target language. Attention mechanisms have shown to improve the
generative network, as detailed in Bahdanau et al. \cite{bahdanau2014neural},
and further work from Luong et al. \cite{luong2015effective}.

We describe the underlying components used to build these sub-networks, and
their advantages and disadvantages.
\subsubsection{RNN}
The simplest way to process sequences of inputs is to use recurrent layers,
which accept an input and previous hidden state to update its output.
Our baseline models incorporate the \emph{Elman RNN}, the simplest recurrent
layer.
\begin{equation}
  \label{eq:rnn}
  h_t = \tanh(W_{ih} x_t + b_{ih}  +  W_{hh} h_{t-1} + b_{hh})
\end{equation}
However, the layer is prone to the vanishing gradient problem
on large sequences. In addition, \emph{Elman RNNs} have the lowest capacity
to learn.
\subsubsection{GRU}
Improvements have been made to the baseline RNN to increase capacity and
improve gradient flow over long sequences. \emph{Long Short-Term Memory} (LSTM)
and \emph{Gated Recurrent Units} (GRU) are two of the most popular improvements
to recurrent networks. Both solve the vanishing gradient problem found in
traditional RNN layers, and improve a model's capacity to learn.
\begin{equation}
  \label{eq:gru}
  \begin{split}\begin{array}{ll}
    r_t = \sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{t-1} + b_{hr}) \\
    z_t = \sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{t-1} + b_{hz}) \\
    n_t = \tanh(W_{in} x_t + b_{in} + r_t \circ (W_{hn} h_{t-1}+ b_{hn})) \\
    h_t = (1 - z_t) \circ n_t + z_t \circ h_{t-1} \\
  \end{array}\end{split}
\end{equation}
For our models, we use GRUs, as they have been shown to perform as well as
LSTMs but with less parameters.
\subsubsection{Bidirectional GRU}
GRUs, and RNNs in general, only process sequences in a forward direction.
However, encoding both the preceding and succeeding input would be beneficial
to learning the context of a word in its local vicinity. Bidirectional layers
allow models to encode inputs in both the forward and reverse direction. This
is a simple, yet poweful, extension to uni-directional recurrent layers.
\begin{equation}
  \label{eq:bidirectional}
  \begin{split}
    \begin{array}{ll}
      \overrightarrow{h_f} = \operatorname{GRU}(\overrightarrow{input})\\
      \\
      \overleftarrow{h_b} = \operatorname{GRU}(\overleftarrow{input})\\
      \\
      h_o = \overrightarrow{h_f} \,\,\Vert \,\, \overrightarrow{h_b}\\
    \end{array}
  \end{split}
\end{equation}
Bidirectional layers run two separate recurrent layers, one on the forward input
sequence; the other on the reversed input. Thus we get two output tensors, one
for each direction. We reverse the backward tensor, and stack the two output
tensors together. Thus our final tensor encodes the forward and backward
context.
\subsection{Encoders}
The encoder, or \emph{inference network}, receives an input token sequence
$\vec{x} = \left[{x_1,\hdots, x_n}\right]$ of length $n$ and processes
this to create an output encoding. The result is a sequence
$\vec{h} = \left[{h_1, \cdots, h_{n}}\right]$ that
maps to the input sequence $\vec{x}$. The final hidden state is $h_n$.

Encoders share the same architecture except for the recurrent layer.
For each input word $x_j$, the encoder looks up the associated word embedding,
and runs the sequence through the recurrent layer. We define $W_e$ as the word
embedding, with each row corresponding to an input token.
\begin{equation}
  \vec{h} = \begin{cases}
    \operatorname{RNN} (W_e[\vec{x}]) \\
    \operatorname{GRU} (W_e[\vec{x}]) \\
    \operatorname{BiGRU} (W_e[\vec{x}]) \\
  \end{cases}
\end{equation}
The output of the recurrent layer is our encodings, and we propagate
the layer's last hidden state to the decoder. Theoretically, the hidden
state should encode all the important information from the input sequence
and pass it along to the decoder, but the
encodings can be used with attention to provide better models. If the encoder
type is bidirectional, then we return only the forward hidden state.

We use three types of encoders: RNN, GRU, and the Bidirectional GRU (BiGRU).
\subsection{Decoders}
The decoder, or \emph{generative network}, receives the encoder outputs, the
model's hidden state, and the last input token. On the first pass of the
decoder, the hidden state is the encoder's final hidden state, and the
input token is SOS.

We can describe our decoders in two ways: with and without attention. For
our recurrent layers, we use the RNN and GRU layers. We cannot use a
Bidirectional GRU because we do not know the full decoded sequence
(and hence why we are decoding).
\begin{equation}
  \tau_i = \begin{cases}
    W_e[t_i] & \emph{No Attention} \\
    W_e[t_i] \,\, \Vert \,\, c_i & \emph{With Attention} \\
  \end{cases}
\end{equation}

In the case of attention, we apply one of the attention schemes (described in
the next section) to the encoder output, given our current decoder's
hidden state. We concatenate the attention results, known as a context vector
$c_i$ with our input embedding for an input token $t_i$.

Without attention, we just use the input embedding and ignore the encoder
outputs.
\begin{equation}
  \mathbf{y} = W_d \cdot \begin{cases}
      \operatorname{RNN} (\tau_i, \, s_{i-1}) \\
      \operatorname{GRU} (\tau_i, \, s_{i-1}) \\
    \end{cases}
\end{equation}
We apply a recurrent layer to the attended tensor, using the hidden state
provided. After applying a linear layer with weights $W_d$ and applying a log
softmax, we output our result for evaluation, which is a log probability
distribution across the target vocabulary.
\begin{equation}
  \log \sigma(\mathbf{y}) = \log \frac{\exp(y_i)}{\sum_j \exp(y_j)}
\end{equation}
\subsection{Attention}
Attention mechanisms have been shown to improve sequence to sequence
translations from Bahdanau et al. \cite{bahdanau2014neural}, and further work
from Luong et al. \cite{luong2015effective} examines global vs local approaches
to attention-based encoder-decoders. Common attention mechanisms are:
\begin{equation}
    score(s_{i-1}, h_j) =
    \begin{cases}
        v_a^T \cdot \tanh (W_a [s_{i-1} \| h_j]) & \text{\emph{concat}} \\
        s_{i-1}^T \cdot W_a \cdot h_j & \text{\emph{general}} \\
        s_{i-1}^T \cdot h_j & \text{\emph{dot}} \\
    \end{cases}
\end{equation}
where $s_{i-1}$ is the previous decoder hidden state, and $h_j$ is the $j$th
encoder output.
To compute scores, which are used to create attention weights, we apply a
softmax:
\begin{equation}
  a(s_{i-1}, h_j) = \frac{\exp(score(s_{i-1}, h_j))}{\sum_{j'}\exp(score(s_{i-1}, h_{j'}))}
\end{equation}
Using these scores, we create a context vector $c_i$, which is just the batch
matrix multiplication between our attention weights and the encoder outputs.
\begin{equation}
  c_i = \sum_{j'} a(s_{i-1}, h_{j'}) \cdot h_{j'}
\end{equation}
We will focus on general attention and concat attention in our
experiments.
\subsection{Teacher-Forcing}
In terms of training, an encoder-decoder system can either accept the target
token or the model's prediction as the next input during the decoding step.
When we use the target token, this is known as teacher forcing, and is
shown to be favored during initial training iterations but should be backed
off to use the model's own predicitons. Continuous use of teacher forcing
without backing off to the model's own decoded tokens will exhibit instability
in the translations as shown in Lamb et al. \cite{lamb2016professor}

During training, we randomly determine whether to feed in the true target
token instead of the generated token to the decoder. This is a configurable
parameter that deterimes the threshold to allow teacher forcing, where 0 means
no teacher forcing and 1 is constant teacher forcing.
%------------------------------------------------
\section{Implementation}
\label{sec:implementation}
\subsection{Batching}
\subsection{GPU/CPU}
\subsection{Metrics}
\subsubsection{Masked NLL Loss}
\label{sec:masked-loss}
\subsubsection{BLEU Scores}
\label{sec:bleu}
We use the Bilingual Evaluation Understudy (BLEU) score as one of our criteria
for the quality of model translations. Outputs are in the range [0, 1], and a
BLEU score of 1 indicates a perfect translation to the reference texts.
We do not need not obtain a perfect score in order to have a good
translation model.

BLEU scores are calculated for individual sentences and averaged across the
whole corpus. Intelligibility and grammatical correctness are not taken into
account when calculating BLEU scores. According to the original paper by
Papineni et al. \cite{papineni2002bleu}, BLEU scores are computed as follows:

\begin{equation}
  \operatorname{BLEU} = \operatorname{BP} \cdot \exp \left( \sum_{n=1}^N w_n \log p_n \right)
\end{equation}

\noindent
where $p_n$ is the geometric average of the modified $n$-gram precisions
and $\operatorname{BP}$ is the brevity penalty, explained below.
(\cite{papineni2002bleu} uses $N=4$, uniform weights $w_n = \frac{1}{N}$.)

The brevity penalty $\operatorname{BP}$ is used for translations that are
too short compared to the reference. BP is defined as:

\begin{equation}
  \operatorname{BP} = \begin{cases}
    1 & \text{if } c > r \\
    e^{(1-r/c)} & \text{if } c \leq r
  \end{cases}
\end{equation}
\noindent
with effective reference corpus length $r$ and candidate translation length $c$.

BLEU scores are non-differentiable, and therefore we cannot directly optimize
on this metric, but can use it to enhance our understanding regarding the
quaity of our models given the data.

%------------------------------------------------
\section{Results: Shakespeare-English}
\label{sec:results}
%------------------------------------------------
\section{Style-Transfer:\\English-Shakespeare}
\label{sec:style-transfer}
%------------------------------------------------
\section{Conclusion \& Future Work}
\label{sec:conclusion}
%------------------------------------------------








% OLD STUFF, SLOWLY MOVE INTO TOP IF NEEDED, CHANGE TENSE




%------------------------------------------------

\section{Architectures}
% \label{sec:architecture}
% go more detailed? or can they not handle the math lol
% these descriptions are good starters, but we can be mroe detailed.

% IDEA on subsectioning:

\subsection{Decoders}
We will describe the decoder algorithm, and experiment with two different
recurrent layers: RNN and GRU. We cannot use a Bidirectional GRU because
we do not know the full decoded sequence (and hence why we are decoding).

Decoders take in the last translated token, starting with an SOS token on a
new batch. It applies an embedding layer, followed by an optional dropout.

We then have two options:
\begin{enumerate}
  \item Attention (General, Concat, etc.)
  \item No Attention
\end{enumerate}
In the case of attention, we apply one of the attention schemes (described in
the next section) to the encoder output, given our current decoding
hidden state. We concatenate the attention results with our input embedding.

Without attention, we just use the input embedding and ignore the encoder
outputs.

We apply a recurrent layer to the attended tensor, using the hidden state
provided. After applying a linear layer and log softmax, we output our result
for evaluation.

\subsection{Attention Mechanisms}
% \label{sec:attention}
Attention mechanisms have been shown to improve sequence to sequence
translations from Bahdanau et al. \cite{bahdanau2014neural}, and further work
from Luong et al. \cite{luong2015effective} examines global vs local approached
to attention-based encoder-decoders. Common attention mechanisms are:
% we will probably use dot & concat attention
\begin{equation}
    score(h_t, h_s) =
    \begin{cases}
        v_a^T \cdot \tanh (W_a [h_t \| h_s]) & \text{\emph{concat}} \\
        h_t^T \cdot W_a \cdot h_s & \text{\emph{general}} \\
        h_t^T \cdot h_s & \text{\emph{dot}} \\
    \end{cases}
\end{equation}
where $h_t$ is the current target hidden state, and $h_s$ is the encoder output.
To compute scores, which are used to create attention weights, we apply a
softmax:
\begin{equation}
  a_t(s) = \frac{\exp(score(h_t, h_s))}{\sum_{s'}\exp(score(h_t, h_{s'}))}
\end{equation}
Using these scores, we create a context vector, which is just the batch matrix
multiplication between our attention weights and the encoder outputs.
We will focus on general attention and concat attention in our
experiments.

\subsection{Teacher Forcing}
In terms of training, an encoder-decoder system can either accept the target
token or the model's prediction as input during the decoding step. When we use
the target token, this is known as teacher forcing, and is shown to be favored
during initial training iterations, but should be backed off to use the
model's own predicitons, as it will exhibit instability in the translations
otherwise, as written in Lamb et al. \cite{lamb2016professor}

We hope to build in a system to decay the teacher forcing percentage over time,
instead of our current implementation that checks a random number againist the
hyperparameter. However, we can bypass this effectively by reloading a model
and changing the teacher forcing parameter provided.

%------------------------------------------------
\section{Roadblocks and Problems}
We have hit two major roadblocks since our proposal, and discuss our apporaches
to mediating the issues.

\subsection{GPU/CPU Training Times}
Systems like these take a long time to learn the data, and one of our major
roadblocks is training time. For a normal CPU run, 2 epochs takes 11-30 minutes,
depending on computer specs. This would put our experiments total running time
at over 2-3 days per experiment (18-27 days total). This does not give us much
leeway to verify our
apporach. Hence, we describe in Section ~\ref{sec:gpu} our intial resutls
in GPU compatability and speedups. These timings were done on the Sparknotes
dataset.

%------------------------------------------------

\section{Additional Implementation\\Details}
\subsection{Batching}
In order to faciliate faster training, and less noisy gradients, we felt it
imperative to introduce batching of sentences. We batched similiar sentences
according to source sentence length (encoder input). This allows us to reduce
the number of batches to loop through and take advantage of torch operations.

\subsection{GPU Compatability}
% \label{sec:gpu}
Initial training is slow on a cpu, with a Bidirectional GRU Encoder
+ GRU Decoder + Concat Attention estimated at 11-30 minutes per 2 epochs,
varying on batch size (32 and 128 tested). In order to improve training time,
we have allowed an optional parameter to use a gpu. Inital run on batch size 128
yielded a training speed of 111 seconds per 2 epochs. Hence our experiments will
take only 4-6 days, and can be distributed across multiple GPUs for even less
time.

Our group has an estimated 3 GPUs (Morris:1; Riley:2). This will help us
distribute our experiments across several computers and allow us time to adjust
experiments based upon our initial findings. If need be, we will us Google
credits to fund more experiments.


%------------------------------------------------

% References
\bibliographystyle{abbrv}
\bibliography{final_report}

%------------------------------------------------

\clearpage
\appendix
\onecolumn
\section{Model Architecture Diagrams}
\begin{figure}[ht]
  \centering
  \begin{tikzpicture}

    % Define nodes
    \node[rectangle] (encoder) {Encoder};
    \node[rectangle, below=2cm of encoder] (decoder) {Decoder};
    \node[rectangle, above=0.75cm of encoder, xshift=-1cm] (s) {$source$};
    \node[rectangle, above=0.75cm of encoder, xshift=1cm] (hidden1) {$None$};
    \node[rectangle, below=0.75cm of encoder, xshift=-1cm] (out1) {$encodings$};
    \node[rectangle, below=0.75cm of encoder, xshift=1cm] (hidden2) {$hidden$};
    \node[rectangle, left=0.75cm of decoder] (t) {$token$};
    \node[rectangle, below=0.75cm of decoder, xshift=-1cm] (out2) {$new$ $token$};
    % \node[rectangle, below=0.75cm of decoder, xshift=1cm] (hidden3) {$hidden$};

    % Connect the nodes
    \edge {s, hidden1} {encoder};
    \edge {encoder} {out1, hidden2};
    \edge {out1, t} {decoder};
    \edge {decoder} {out2};
    \edge {out2} {t};
    \path [->] (decoder) edge [bend right=15] node {} (hidden2);
    \path [->] (hidden2) edge [bend right=15] node {} (decoder);

  \end{tikzpicture}
  \caption{Model Architecture Overview for Encoder-Decoder.}
  \label{fig:encoder-decoder}
\end{figure}
\begin{figure}[ht]
  \centering
  \begin{tikzpicture}

    % Define nodes
    \node[rectangle] (rnn) {RNN/GRU};
    \node[rectangle, below=0.75cm of rnn, xshift=-1cm] (out) {$output$};
    \node[rectangle, below=0.75cm of rnn, xshift=1cm] (hidden2) {$hidden$};
    \node[rectangle, above=0.75cm of rnn, xshift=-1cm] (embed) {Embed};
    \node[rectangle, above=0.75cm of embed] (input) {$source$};
    \node[rectangle, right=0.75cm of input] (hidden1) {$hidden$};

    % Connect the nodes
    \edge {input} {embed};
    \edge {embed, hidden1} {rnn};
    \edge {rnn} {out, hidden2};

  \end{tikzpicture}
  \caption{Model Architecture for Encoder.}
  \label{fig:encoder}
\end{figure}
\begin{figure}[ht]
  \centering
  \begin{tikzpicture}

    % Define nodes
    \node[rectangle] (rnn) {RNN/GRU};
    \node[rectangle, above=0.75cm of rnn] (embed) {Embed};
    \node[rectangle, above=0.75 of embed] (input) {$input$};
    \node[rectangle, right=1cm of input] (hidden1) {$hidden$};
    \node[rectangle, below=0.75cm of rnn] (linear) {Linear};
    \node[rectangle, below=0.75cm of linear] (logs) {Log Softmax};
    \node[rectangle, below=0.75cm of logs] (out) {$output$};
    \node[rectangle, right=1.25cm of out] (hidden2) {$hidden$};

    % Connect the nodes
    \edge {input} {embed};
    \edge {embed, hidden1} {rnn};
    \edge {rnn} {linear, hidden2};
    \edge {linear} {logs};
    \edge {logs} {out};

  \end{tikzpicture}
  \caption{Decoder with No Attention.}
  \label{fig:decoder-no-attn}
\end{figure}
\begin{figure}[ht]
  \centering
  \begin{tikzpicture}

    % Define nodes
    \node[rectangle] (rnn) {RNN/GRU};
    \node[rectangle, above=0.75 of rnn, xshift=-1cm] (cat) {Concat};
    \node[rectangle, above=0.75cm of cat] (attn) {Attn};
    \node[rectangle, left=0.75cm of attn] (embed) {Embed};
    \node[rectangle, above=0.75 of embed] (input) {$input$};
    \node[rectangle, above=0.75cm of attn] (encoder) {$encoding$};
    \node[rectangle, right=1cm of encoder] (hidden1) {$hidden$};
    \node[rectangle, below=0.75cm of rnn] (linear) {Linear};
    \node[rectangle, below=0.75cm of linear] (logs) {Log Softmax};
    \node[rectangle, below=0.75cm of logs] (out) {$output$};
    \node[rectangle, right=1cm of out] (hidden2) {$hidden$};

    % Connect the nodes
    \edge {input} {embed};
    \edge {encoder, hidden1} {attn};
    \edge {attn, embed} {cat};
    \edge {cat, hidden1} {rnn};
    \edge {rnn} {linear, hidden2};
    \edge {linear} {logs};
    \edge {logs} {out};

  \end{tikzpicture}
  \caption{Decoder with Attention.}
  \label{fig:decoder-attn}
\end{figure}
\begin{figure}[ht]
  \centering
  \begin{tikzpicture}

    % Define nodes
    \node[rectangle] (out) {$output$};
    \node[rectangle, above=0.75cm of out] (bmm) {BMM};
    \node[rectangle, above=0.75cm of bmm, xshift=-1cm] (soft) {Softmax};
    \node[rectangle, above=0.75 of soft] (lin2) {Linear};
    \node[rectangle, above=0.75 of lin2] (tanh) {$\tanh$};
    \node[rectangle, above=0.75 of tanh] (lin1) {Linear};
    \node[rectangle, above=0.75 of lin1] (cat) {Concat};
    \node[rectangle, above=0.75 of cat, xshift=-1cm] (hidden) {$hidden$};
    \node[rectangle, above=0.75 of cat, xshift=1cm] (encodings) {$encodings$};

    % Connect the nodes
    \edge {encodings, hidden} {cat};
    \edge {cat} {lin1};
    \edge {lin1} {tanh};
    \edge {tanh} {lin2};
    \edge {lin2} {soft};
    \edge {soft, encodings} {bmm};
    \edge {bmm} {out};

  \end{tikzpicture}
  \caption{Concat (Bahdanau) Attention Layer.}
  \label{fig:concat-attn}
\end{figure}
\begin{figure}[ht]
  \centering
  \begin{tikzpicture}

    % Define nodes
    \node[rectangle] (out) {$output$};
    \node[rectangle, above=0.75cm of out] (bmm) {BMM};
    \node[rectangle, above=0.75cm of bmm, xshift=-1cm] (soft) {Softmax};
    \node[rectangle, above=0.75 of soft] (bilinear) {Bilinear};
    \node[rectangle, above=0.75 of bilinear, xshift=-1cm] (hidden) {$hidden$};
    \node[rectangle, above=0.75 of bilinear, xshift=1cm] (encodings) {$encodings$};

    % Connect the nodes
    \edge {encodings, hidden} {bilinear};
    \edge {lin2} {soft};
    \edge {soft, encodings} {bmm};
    \edge {bmm} {out};

  \end{tikzpicture}
  \caption{General Attention Layer.}
  \label{fig:general-attn}
\end{figure}

\end{document}
